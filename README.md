# Flappy Bird Neural Netword

Projeto desenvolvido por [Igor Andrade](https://github.com/andradeigor). Este projeto foi feito como projeto final da disciplina Computa√ß√£o Cient√≠fica e An√°lise de Dados, consiste na implementa√ß√£o de uma rede neural treinada via algoritmo gen√©tico para jogar Flappy Bird.

- [Teoria](#üñã-teoria)
- [Implementa√ß√£o](#üíª-implementa√ß√£o)
- [Como Usar](#ü§ñ-como-usar)
- [Demonstra√ß√£o](#üìú-demonstra√ß√£o)
- [Tecnologias](#üíª-tecnologias)
- [Contribuidores](#üë•-contribuidores)
- [Licen√ßa](#üìñ-licen√ßa)

## üñã Teoria:

## üß† Redes Neurais

Para entender esse projeto precisamos primeiro ter em mente as partes que o comp√µem. Uma parte importante do c√≥digo √© feita via um algoritmo de Template Matching, usando o biblioteca OpenCV2. Esse algoritmo foi implementado e explicado por mim no trabalho final da disciplina [√Ålgebra Linear Algor√≠tmica](https://github.com/andradeigor/CosineMatcher) e est√° sendo usado nesse projeto para detectar e localizar o p√°ssaro e os canos do jogo Flappy Bird.

Com essa informa√ß√£o em m√£os, passamos esses dados para uma Rede Neural que foi implementada do zero, seguindo o seguinte modelo:

![image-2](https://github.com/andradeigor/CosineMatcher/assets/21049910/5b077f70-f4fa-4e78-982f-826e0ca14a62)

Dentro de uma rede neural, n√≥s temos esse n√≥s de entrada que passam a informa√ß√£o que ser√° analizada. No nosso caso, os dados de entrada s√£o a localiza√ß√£o do p√°ssaro e dos canos. Ap√≥s esses dados chegarem na camada de entrada eles s√£o processados pelas camadas internas (hidden layers) e chegam no neur√¥nio de sa√≠da da rede neural, que nos d√° um valor entre [0-1], dependendo do valor que n√≥s foi dado, n√≥s realizamos ou n√£o uma a√ß√£o dentro do jogo.

Na realidade, o que est√° acontecendo na rede neural √© que todos os neur√¥nios das camadas anteriores s√£o conectados aos neur√¥nios da camada seguinte e s√£o usados para gerar seus valores. Para melhor vizualiza√ß√£o, tome como exemplo o neur√¥nio $n_{11}$ da imagem acima, ele possui uma liga√ß√£o com $\{a_1,a_2,a_3\}$ por meio das conex√µes $\{w_1,w_2,w_3\}$. Assim, o que ocorre matematicamente falando √© que o valor de $n_{11}$ √© calculado da seguinte maneira:

$$n_{11} = a_1.w_1+a_2.w_2+a_3.w3$$

Al√©m disso, outro detalhe associado √† cada neur√¥nio √© que cada um deles possuem um valor especial associado chamado de bias, que √© somado ao final de todo c√°lculo feito. Assim, completando o exemplo acima temos:

$$n_{11} = a_1.w_1+a_2.w_2+a_3.w3+b_{11}$$

Esse valor especial em conjunto com os $w_i$ servem para determinar qual ser√° o resultado de cada neur√¥nio dependendo de cada entrada. Indo mais a fundo matematicamente falando, os $w_i$ s√£o os pesos que cada entrada recebe no valor final, e o bias √© um valor que desloca o resultado gerado por essa conta.

Ap√≥s esse c√°lculo ser realizado para um neur√¥nio, ao inv√©s de pegarmos esse resultado e passar para frente, √© adicionaddo mais um tratamento para esse valor gerado: uma fun√ß√£o de ativa√ß√£o √© usada nesse valor, e o resultado dessa fun√ß√£o √© passado como entrada para o pr√≥ximo neur√¥nio, e assim os valores v√£o caminhando pela rede neural.

![image](https://github.com/andradeigor/CosineMatcher/assets/21049910/4945954c-6b02-4fa4-acd7-160ad5be0da3)

Acima, temos um exemplo da fun√ß√£o de ativa√ß√£o usada nesse projeto $y=tanh(x)$. Perceba que ao usarmos uma fun√ß√£o de ativa√ß√£o, n√≥s padronizamos os poss√≠veis resultados de cada neur√¥nio, n√£o permitindo que eles cres√ßam indefinidamente, sempre nos gerando um resultado entre [-1,1]. Al√©m disso, ao usarmos uma fun√ß√£o de ativa√ß√£o ganha-se o poder de modelar sistemas n√£o lineares dentro de uma rede neural, j√° que agora o c√°lculo da rede inteira n√£o mais pode ser definido como um som√°t√≥rio de fun√ß√µes lineares com pesos.

Com isso, finalmente podemos descrever realmente o c√°lculo que ocorre em cada um dos neur√¥nios:

$$n_{11} = tanh(a_1.w_1+a_2.w_2+a_3.w3+b_{11})$$

Generalizando o c√°lculo acima, temos:

$$n_{ij} = act((\sum_{k=1}^N a_{k}.w_{kj}) +  b_{ij})$$

Onde $N$ √© o n√∫mero de neur√¥nios na camada anterior, $w_{kj}$ s√£o os pesos associados aos neur√¥nios anteriores $a_k$, $b_{ij}$ √© o bias asssociado com o neur√¥nio $n_{ij}$ e act √© a fun√ß√£o de ativa√ß√£o escolhida.

## üß¨ Algoritmo Gen√©tico

Al√©m de redes neurais, o projeto tamb√©m trata da implementa√ß√£o de um algoritmo gen√©tico para o treinamento, diferente da abordagem padr√£o de se usar Backpropagation. Esse algoritmo se baseia no processo de sele√ß√£o natural.

- O processo se d√° em um ciclo, no qual inicialmente geramos aleatoriamente uma s√©rie de "indiv√≠duos" (no nosso caso, redes neurais). Esses indiv√≠duos s√£o avaliados em um crit√©rio heur√≠stico feito para o problema em quest√£o e no final eles recebem uma nota baseada no qu√£o bem eles foram.

- Como esses indiv√≠duos foram gerados aleatoriamente, dificilmente eles v√£o se sair bem de primeira. Mas, alguns v√£o, por sorte, se sair melhor que os outros. Esses ind√≠viduos que tiveram a melhor pontua√ß√£o ser√£o escolhidos para servirem como "base" para a nova popula√ß√£o.

- Ap√≥s selecionados, passamos para a etapa de reprodu√ß√£o, na qual o "DNA" dos selecionados √© usado de base para criar a nova popula√ß√£o, pegando um peda√ßo de cada um deles de forma aleat√≥ria e compondo os novos. Ao final desse processo, visando fugir de m√≠nimos locais, √© tamb√©m adicionada muta√ß√µes aos DNA's criados, com uma chance bem baixa de ocorrer.

Por fim, essa popula√ß√£o nova √© avaliada e o ciclo se repete, como ilustra bem a imagem abaixo:

![genetic](https://github.com/andradeigor/CosineMatcher/assets/21049910/32eacce2-bd98-4760-81f0-d3ca67a0e1ad)

## üíª Implementa√ß√£o:

Nosso objetivo √© implementar uma rede neural que √© treinada com base no algoritmo gen√©tico, o inicio do projeto foi implementar as opera√ß√µes b√°sicas da rede neural. Para atingir esse objetivo, come√ßou-se implementando um objeto inicial chamado de Layer que representa uma camada inteira da rede neural, com $n$ neur√¥nios e realiza a opera√ß√£o de passar os dados por por eles.

Para isso, representamos essa camada como uma matriz $w_{nxm}$, onde $n$ √© o n√∫mero de neur√¥nios e $m$ √© o n√∫mero de pesos que cada neur√¥nio possui, que coincide com o n√∫mero de neur√¥nios presentes na camada anterior.

Assim, dado um vetor $i_{1xm}$ : $[i_1,i_2,...,i_m]$ que cont√™m o input dessa camada, podemos realizar os c√°lculos dessa camada como:

$$resultado = i_{1xm}.w_{nxm}^T = 
[\sum_{j=1}^m a_{1}.w_{1j},\sum_{j=1}^m a_{2}.w_{2j},...,\sum_{j=1}^m a_{m}.w_{mj}]$$

Agora, para que essa opera√ß√£o resulte nos c√°lculos que cada camada precisa fazer, basta somarmos um vetor $b_{1xm}$ que possui as biases de cada neur√¥nio:

$$resultado = i_{1xm}.w_{nxm}^T + b_{1xm} =
[\sum_{j=1}^m a_{1}.w_{1j} + b_{1},\sum_{j=1}^m a_{2}.w_{2j} + b_{2},...,\sum_{j=1}^m a_{m}.w_{mj} + b_{m}]$$

Esse m√©todo foi implementado no objeto chamado de Layer da seguinte forma:

```python
def foward(self, input):
        self.output = np.dot(input,self.weights.T) + self.biases
```

Junto √† m√©todo tamb√©m foram adicionados um m√©todo de constru√ß√£o que gera valores aleat√≥rios para os pesos, bem como uma fun√ß√£o de ativa√ß√£o para gerar o resultado do c√°lculos. Ficando assim com:

```python
class Layer:
    def __init__(self,nInput=None,nNeurons=None):
        if(nInput==None or nNeurons==None): return
        self.weights = np.array(0.2 * np.random.randn(nNeurons,nInput))
        self.biases = np.array(0.2 * np.random.randn(1,nNeurons))
    def foward(self, input):
        self.output = np.dot(input,self.weights.T) + self.biases

    def tanh(self,values):
        self.result = np.tanh(values)

```

Com basse nessa classe e uma outra classe de Rede Neural que √© capaz de criar v√°rias camadas e fazer o c√°lculo para cada uma delas, foi poss√≠vel implementar completamente a rede neural.

## üìú Demonstra√ß√£o:

## üíª Tecnologias

- Python
- Numpy
- OpenCV
- Pygame
- mss
- pynput

## üë• Contribuidores

Esses s√£o os contribuidores do projeto (<a href="https://allcontributors.org/docs/en/emoji-key">emoji key</a>).

<table>
  <tr>
    <td align="center"><a href="https://github.com/andradeigor"><img style="border-radius: 50%;" src="https://avatars.githubusercontent.com/u/21049910?v=4" width="100px;" alt=""/><br /><sub><b>Igor Andrade</b></sub></a><br /><a href="https://github.com/andradeigor/DiscordBotUFRJ/commits?author=andradeigor" title="Igor Andrade">ü§î üíª üöß</a></td>
  </tr>
</table>

## üìñ Licen√ßa

Este projeto est√° licenciado sob a licen√ßa <a href="https://choosealicense.com/licenses/mit/">MIT</a>.
